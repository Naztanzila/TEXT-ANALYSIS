# -*- coding: utf-8 -*-
"""DA-Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P0OGcQ19_wUSc5bM_P5PDO1zqzbr56oX
"""

pip install beautifulsoup4 openpyxl nltk

from google.colab import files

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import requests
from bs4 import BeautifulSoup
import openpyxl
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
from textblob import TextBlob
import csv
import numpy as np

# extract_content function
def extract_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extracting title and text
    title = soup.title.text if soup.title else ''
    article_text = ' '.join([p.text for p in soup.find_all('p')])

    return title, article_text

url = "https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/"

# Applying the function on the specified URL
title, article_text = extract_content(url)

print("Title:", title)
print("Article Text:", article_text)

!pip install xlrd
!pip install nltk

nltk.download('vader_lexicon')

from openpyxl import load_workbook

input = load_workbook('/content/drive/MyDrive/Untitled folder/20211030 Test Assignment/Input.xlsx')
sheet = input.active

rows = sheet.rows
print(type(rows))

# Open output Excel file
output = load_workbook('/content/drive/MyDrive/Untitled folder/20211030 Test Assignment/Output Data Structure.xlsx')
output_sheet = output.active

output_sheet

def analyze_sentiment(text):
    sia = SentimentIntensityAnalyzer()
    sentiment_scores = sia.polarity_scores(text)
    return sentiment_scores['compound']

for row in sheet.iter_rows(min_row=2, values_only=True):
    url_id, url = row[0], row[1]

    # Extract content from the URL
    title, article_text = extract_content(url)

    # Analyze sentiment
    sentiment = analyze_sentiment(article_text)

    print(f"URL ID: {url_id}, URL: {url}, Sentiment: {sentiment}")

# Write results to the output Excel file
    output_sheet.append([url_id, url, sentiment])

nltk.download('stopwords')

df = pd.read_csv('/content/Input2.xlsx - Sheet1.csv',index_col=0)

df

li = [url for url in df['URL']]

li

import bs4 as bfs

text_list = []

for url in li:
    headers = {"User-Agent": "XY"}

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # Prettifying the HTML
        text_list.append(soup.prettify())
    else:
        print(f"Failed to fetch content from {url}. Status code: {response.status_code}")

print(len(text_list))

print(type(text_list))

articles = []
for element in text_list:
    soup = BeautifulSoup(element, 'html.parser')
    articles_from_page = soup.find_all(attrs={"class": "td-post-content"})
    articles.extend(articles_from_page)

print(len(articles))

!pip install BeautifulSoup4
from bs4 import BeautifulSoup

import nltk

nltk.download('punkt')

!pip install fake-useragent

import numpy as np
import re
import os
from nltk.tokenize import RegexpTokenizer , sent_tokenize
from urllib.request import urlopen
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import urllib.request,sys,time ,requests

stopWordsFile =     '/content/drive/MyDrive/Untitled folder/20211030 Test Assignment/StopWords/StopWords_Generic.txt'
positiveWordsFile = '/content/drive/MyDrive/Untitled folder/20211030 Test Assignment/MasterDictionary/positive-words.txt'
nagitiveWordsFile = '/content/drive/MyDrive/Untitled folder/20211030 Test Assignment/MasterDictionary/negative-words.txt'

def get_article_names(urls):
  titles = []
  for i in range (len(urls)):
    title = urls[i]
    title_clean = title[title.index( "m/" ) + 2 :-1]. replace('-' , ' ')
    titles.append(title_clean)
  return titles

urls =df["URL"]
urlsTitleDF = get_article_names(urls)
urlsTitleDF

url = "https://insights.blackcoffer.com/how-people-diverted-to-telehealth-services-and-telemedicine"

page=requests.get(url , headers={"User-Agent": "XY"})
soup = BeautifulSoup(page.text , 'html.parser')

#get title
title = soup . find("h1",attrs = { 'class' : 'entry-title'}).get_text()

text = soup . find(attrs = { 'class' : 'td-post-content'}).get_text()
lines = (line.strip() for line in text.splitlines())
chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
text = '\n'.join(chunk for chunk in chunks if chunk)

# Loading positive words
with open(positiveWordsFile,'r') as posfile:
    positivewords=posfile.read().lower()
positiveWordList=positivewords.split('\n')

# Loading negative words
with open(nagitiveWordsFile ,'r' ,  encoding="ISO-8859-1") as negfile:
    negativeword=negfile.read().lower()
negativeWordList=negativeword.split('\n')

#Loading stop words dictionary for removing stop words

with open(stopWordsFile ,'r') as stop_words:
    stopWords = stop_words.read().lower()
stopWordList = stopWords.split('\n')
stopWordList[-1:] = []

display( positiveWordList[:6]  , negativeWordList[:6] , stopWordList[:6])

#tokenizeing module and filtering tokens using stop words list, removing punctuations
def tokenizer(text):
    text = text.lower()
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(text)
    filtered_words = list(filter(lambda token: token not in stopWordList, tokens))
    return filtered_words

def positive_score(text):
    posword = 0
    tokenphrase = tokenizer(text)
    for word in tokenphrase:
        if word in positiveWordList:
            posword += 1

    return posword

def negative_score(text):
    negword = 0
    tokenphrase = tokenizer(text)
    for word in tokenphrase:
        if word in negativeWordList:
            negword += 1

    return negword

def polarity_score(positive_score, negative_score):
    return (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)

def total_word_count(text):
    tokens = tokenizer(text)
    return len(tokens)

def AverageSentenceLenght(text):
    Wordcount = len(tokenizer(text))
    SentenceCount = len(sent_tokenize(text))

    if SentenceCount > 0:
        Average_Sentence_Lenght = Wordcount / SentenceCount
        avg = Average_Sentence_Lenght
    else:
        avg = None

    return round(avg) if avg is not None else avg

def complex_word_count(text):
    tokens = tokenizer(text)
    complexWord = 0

    for word in tokens:
        lowercase_word = word.lower()


        if lowercase_word.endswith(('es', 'ed')):
            pass
        else:
            vowels = sum(1 for char in lowercase_word if char in 'aeiou')

            if vowels > 2:
                complexWord += 1

    return complexWord

def percentage_complex_word(text):
    tokens = tokenizer(text)
    complexWord = 0
    complex_word_percentage = 0

    if len(tokens) > 0:
        for word in tokens:
            lowercase_word = word.lower()
            if lowercase_word.endswith(('es', 'ed')):
                pass
            else:
                vowels = sum(1 for char in lowercase_word if char in 'aeiou')
                if vowels > 2:
                    complexWord += 1

        complex_word_percentage = complexWord / len(tokens)

    return complex_word_percentage

import math

def fog_index(averageSentenceLength, percentageComplexWord):
    fogIndex = 0.4 * (averageSentenceLength + 100 * percentageComplexWord)
    return math.sqrt(fogIndex)

pip install textblob

from textblob import TextBlob
from bs4 import BeautifulSoup
import requests

def subjectivity_score(text):
    blob = TextBlob(text)
    return blob.sentiment.subjectivity

subjectivity_scores = []

for url in li:
    headers = {"User-Agent": "XY"}

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        prettified_html = soup.prettify()
        score = subjectivity_score(prettified_html)
        subjectivity_scores.append(score)
    else:
        print(f"Failed to fetch content from {url}. Status code: {response.status_code}")

pip install syllables

URLS = df ["URL"]
URLS

print(title)

print(page)

print(soup)

print(url)

print(page.text)

# Search by class
h1_by_class = soup.find("h1", class_="entry-title")
print(h1_by_class)

# Search by tag name
h1_by_tag = soup.find("h1")
print(h1_by_tag)

h1_elements = soup.find_all("h1")
if h1_elements:
    title = h1_elements[0].get_text()

h1_elements

h1_element = soup.find("h1", attrs={"class": "tdb-title-text"})
print(h1_element)

title = soup.find("h1", attrs={'class': 'entry-title'})
if title is not None:
  title = title.get_text()
else:
  print(f"No title found in {urls}")

text = soup.find(attrs={'class': 'td-post-content'})
if text is not None:
  text = text.get_text()
else:
  print(f"No article text found in {urls}")

corps = []

for url in URLS:
    page = requests.get(url, headers={"User-Agent": "XY"})
    soup = BeautifulSoup(page.text, 'html.parser')
    title_tag = soup.find("h1", attrs={'class': 'entry-title'})
    title = title_tag.get_text() if title_tag else "No title found"

    # article text
    text_tag = soup.find(attrs={'class': 'td-post-content'})
    text = text_tag.get_text() if text_tag else "No article content found"

    # breaking into lines
    lines = (line.strip() for line in text.splitlines())
    # multi-headlines into a line
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    corps.append({"url": url, "title": title, "text": text})

print(corps)

import syllables

df = pd.DataFrame({'title': urlsTitleDF, 'corps': corps})

df["text"] = df["corps"].apply(lambda x: x.get("text", ""))
df["total word count"] = df["text"].apply(total_word_count)
df["percentage_complex_word"] = df["text"].apply(percentage_complex_word)
df["complex_word_count"] = df["text"].apply(complex_word_count)
df["AverageSentenceLenght"] = df["text"].apply(AverageSentenceLenght)
df["positive_score"] = df["text"].apply(positive_score)
df["negative_score"] = df["text"].apply(negative_score)
df["subjectivity_score"] = df["text"].apply(subjectivity_score)
df["syllable_count"] = df["text"].apply(lambda x: sum(syllables.estimate(s) for s in x.split()))
df["polarity_score"] = np.vectorize(polarity_score)(df['positive_score'], df['negative_score'])
df["fog_index"] = df.apply(lambda row: fog_index(row["AverageSentenceLenght"], row["percentage_complex_word"]), axis=1)

df

df.to_excel('Output Data Structure.xlsx', encoding='utf-8')

